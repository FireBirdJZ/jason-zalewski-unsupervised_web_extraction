{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "profs = ['zaher', 'sadve', 'vadve', 'agha', 'n-ahuja', 'ramn', 'alawini', 'namato', 'maa', 'angrave', 'bpbailey', 'arindamb', 'batesa', 'mattox', 'spbhat2', 'clblake', 'nikita', 'tbretl', 'bigdog', 'caesar', 'rhc', 'chackoge', 'challen', 'tmc', 'karthe', 'eshwar', 'kcchang', 'chekuri', 'dchen', 'wendycho', 'girishc', 'camillec', 'bcosman', 'katcun', 'rcunnin2', 'davis68', 'mrebl', 'delgosha', 'jdiesner', 'minhdo', 'krdc', 'dullerud', 'melkebir', 'jeffe', 'gcevans', 'waf', 'fischerp', 'mfleck', 'cwfletch', 'miforbes', 'daf', 'friedman', 'aganesn2', 'jugal', 'ygertner', 'ghose', 'girju', 'pbg', 'mgolpar', 'wgropp', 'kaiyug', 'lgui', 'cgunter', 'egunter', 'indy', 'saurabhg', 'hanj', 'harandi', 'sariel', 'jch', 'jhasegaw', 'kkhauser', 'jingrui', 'daheath', 'heath', 'glherman', 'juliahmr', 'dhoiem', 'yihchun', 'jianh', 'yunhuang', 'w-hwu', 'rkiyer', 'reyhaneh', 'shj', 'hengji', 'nanjiang', 'kale', 'kamin', 'ddkang', 'kkarahal', 'dskatz', 'dakshita', 'nskim', 'kindrtnk', 'kirlik', 'andreask', 'wtkramer', 'rhk', 'emer-kuck', 'rakeshk', 'ranjitha', 'hclane', 'lavalle', 'lawrie', 'slazebni', 'klevchen', 'colleenl', 'lbo', 'yunzhuli', 'hl314', 'j-liu1', 'ludaesch', 'lumetta', 'marinov', 'mchenry', 'rutameht', 'charithm', 'meseguer', 'mickunas', 'milenkov', 'soc1024', 'misailo', 'mitras', 'radhikam', 'moralesa', 'klara', 'ymn', 'dmnicol', 'mnowak1', 'idoia', 'lukeo', 'padua', 'yongjoo', 'madhu', 'sjp', 'haopeng', 'jianpeng', 'pitt', 'maxim', 'rwerger', 'jrehg', 'j-reid1', 'reingold', 'renling', 'tringer', 'generobi', 'grosu', 'croy', 'schatz', 'aschwing', 'lrs', 'shaffer1', 'mfsilva', 'ggnds', 'msinha', 'skeel', 'paris', 'snir', 'bradsol', 'solomon2', 'elahe', 'rsrikant', 'ssterman', 'jimeng', 'hs1', 'rubyt', 'mjt', 'htong', 'torrella', 'twidale', 'luthert', 'varshney', 'deepakv', 'pramodv', 'vmahesh', 'gangw', 'shaowen', 'shenlong', 'yvw', 'yxw', 'warnow', 'tiffani', 'winslett', 'mwoodley', 'arwool', 'tyxu', 'yuanwz', 'czhai', 'lingming', 'hanzhao', 'zilles']\n",
    "# test\n",
    "#profs = ['jeffe', 'vadve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m h2_headers \u001b[39m=\u001b[39m [(\u001b[39m\"\u001b[39m\u001b[39mEducation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEducation\u001b[39m\u001b[39m\"\u001b[39m), (\u001b[39m\"\u001b[39m\u001b[39mAcademic Positions\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAcademic Positions\u001b[39m\u001b[39m\"\u001b[39m), (\u001b[39m\"\u001b[39m\u001b[39mResearch Interests\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mResearch Interests\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# send a GET request to the webpage\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# parse the HTML content of the webpage using Beautiful Soup\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/api.py:76\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m'\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:542\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    537\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    538\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    539\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    540\u001b[0m }\n\u001b[1;32m    541\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 542\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    544\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/sessions.py:655\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    654\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    657\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    658\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests/adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 439\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    440\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    441\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    442\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    443\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    444\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    445\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    446\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    447\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    448\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    449\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    450\u001b[0m         )\n\u001b[1;32m    452\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# specify the URL of the webpage to be scraped\n",
    "for prof in profs:\n",
    "    url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "\n",
    "    # specify the list of h2 headers to search for and the corresponding text labels\n",
    "    h2_headers = [(\"Education\", \"Education\"), (\"Academic Positions\", \"Academic Positions\"), (\"Research Interests\", \"Research Interests\")]\n",
    "\n",
    "    # send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content of the webpage using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # extract the name, department, and university from the webpage title\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        name = title_parts[0]\n",
    "        department = title_parts[1]\n",
    "        university = title_parts[2]\n",
    "    else:\n",
    "        print('No title tag found')\n",
    "        name = ''\n",
    "        department = ''\n",
    "        university = ''\n",
    "\n",
    "    # extract the email from the webpage\n",
    "    email = soup.find('div', class_='email')\n",
    "    if email:\n",
    "        email = email.a.text.strip()\n",
    "    else:\n",
    "        email = ''\n",
    "\n",
    "    # create a string to hold the information of the new person\n",
    "    person_info = f\"Name: {name}\\nDepartment: {department}\\nUniversity: {university}\\nEmail: {email}\\n\\n\"\n",
    "    for header_text, label in h2_headers:\n",
    "        # find the h2 header with the specified text\n",
    "        h2 = soup.find('h2', text=header_text)\n",
    "        if h2:\n",
    "            # find the ul tag after the h2 header\n",
    "            ul = h2.find_next('ul')\n",
    "            if ul:\n",
    "                # loop through the li tags in the ul tag and add them to the person_info string\n",
    "                for li in ul.find_all('li'):\n",
    "                    person_info += f\"{label}: {li.text.strip()}\\n\"\n",
    "            else:\n",
    "                person_info += f\"{label}: Not Found\\n\"\n",
    "                print('No ul tag found after header:', header_text)\n",
    "        else:\n",
    "            person_info += f\"{label}: Not Found\\n\"\n",
    "            print('Header not found:', header_text)\n",
    "\n",
    "    # write the person_info string to a text file in append mode\n",
    "    with open('people.txt', 'a') as file:\n",
    "        file.write(person_info + \"\\n<----------------------------------------------------------------->\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = ['mikaelb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Research Interests\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n",
      "Header not found: Academic Positions\n",
      "Header not found: Education\n",
      "Header not found: Academic Positions\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# specify the URL of the webpage to be scraped\n",
    "for prof in profs:\n",
    "    url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "\n",
    "    # specify the list of h2 headers to search for and the corresponding text labels\n",
    "    h2_headers = [(\"Education\", \"Education\"), (\"Academic Positions\", \"Academic Positions\"), (\"Research Interests\", \"Research Interests\")]\n",
    "\n",
    "    try:\n",
    "        # send a GET request to the webpage\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        print('Did not find:', url)\n",
    "        continue\n",
    "\n",
    "    # parse the HTML content of the webpage using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # extract the name, department, and university from the webpage title\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        name = title_parts[0]\n",
    "        department = title_parts[1]\n",
    "        university = title_parts[2]\n",
    "    else:\n",
    "        print('No title tag found')\n",
    "        name = ''\n",
    "        department = ''\n",
    "        university = ''\n",
    "\n",
    "    # extract the email and phone number from the webpage\n",
    "    email_div = soup.find('div', class_='email')\n",
    "    email = email_div.a.text if email_div else ''\n",
    "    phone_div = soup.find('div', class_='phone')\n",
    "    phone = phone_div.text.strip() if phone_div else ''\n",
    "    office_div = soup.find('div', class_='office')\n",
    "    office = office_div.text.strip() if office_div else ''\n",
    "\n",
    "    # create a string to hold the information of the new person\n",
    "    person_info = f\"Name: {name}\\nDepartment: {department}\\nUniversity: {university}\\nEmail: {email}\\nPhone: {phone}\\nOffice: {office}\\n\\n\"\n",
    "    for header_text, label in h2_headers:\n",
    "        # find the h2 header with the specified text\n",
    "        h2 = soup.find('h2', text=header_text)\n",
    "        if h2:\n",
    "            # find the ul tag after the h2 header\n",
    "            ul = h2.find_next('ul')\n",
    "            if ul:\n",
    "                # loop through the li tags in the ul tag and add them to the person_info string\n",
    "                for li in ul.find_all('li'):\n",
    "                    person_info += f\"{label}: {li.text.strip()}\\n\"\n",
    "            else:\n",
    "                person_info += f\"{label}: Not Found\\n\"\n",
    "                print('No ul tag found after header:', header_text)\n",
    "        else:\n",
    "            person_info += f\"{label}: Not Found\\n\"\n",
    "            print('Header not found:', header_text)\n",
    "\n",
    "    # write the person_info string to a text file in append mode\n",
    "    with open('people.txt', 'a') as file:\n",
    "        file.write(person_info + \"\\n<----------------------------------------------------------------->\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header not found: Research Interests\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# specify the URL of the webpage to be scraped\n",
    "for prof in profs:\n",
    "    url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "\n",
    "    # specify the list of h2 headers to search for and the corresponding text labels\n",
    "    h2_headers = [(\"Education\", \"Education\"), (\"Academic Positions\", \"Academic Positions\"), (\"Research Interests\", \"Research Interests\")]\n",
    "\n",
    "    # send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content of the webpage using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # extract the name, department, and university from the webpage title\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        name = title_parts[0]\n",
    "        department = title_parts[1]\n",
    "        university = title_parts[2]\n",
    "    else:\n",
    "        print('No title tag found')\n",
    "        name = ''\n",
    "        department = ''\n",
    "        university = ''\n",
    "\n",
    "    # create a list to hold the information of the new person\n",
    "    person_info = [name, department, university]\n",
    "\n",
    "    # loop through the h2 headers and add the corresponding information to the list\n",
    "    for header_text, label in h2_headers:\n",
    "        # find the h2 header with the specified text\n",
    "        h2 = soup.find('h2', text=header_text)\n",
    "        if h2:\n",
    "            # find the ul tag after the h2 header\n",
    "            ul = h2.find_next('ul')\n",
    "            if ul:\n",
    "                # loop through the li tags in the ul tag and add them to the person_info list\n",
    "                info_list = []\n",
    "                for li in ul.find_all('li'):\n",
    "                    info_list.append(li.text.strip())\n",
    "                person_info.append('\\n'.join(info_list))\n",
    "            else:\n",
    "                person_info.append('Not Found')\n",
    "                print('No ul tag found after header:', header_text)\n",
    "        else:\n",
    "            person_info.append('Not Found')\n",
    "            print('Header not found:', header_text)\n",
    "\n",
    "    # find the email and add it to the list\n",
    "    email_div = soup.find('div', {'class': 'email'})\n",
    "    if email_div:\n",
    "        email = email_div.a.text\n",
    "    else:\n",
    "        email = 'Not Found'\n",
    "        print('Email not found')\n",
    "    person_info.append(email)\n",
    "\n",
    "    # find the phone number and add it to the list\n",
    "    phone_div = soup.find('div', {'class': 'phone'})\n",
    "    if phone_div:\n",
    "        phone = phone_div.text.strip()\n",
    "    else:\n",
    "        phone = 'Not Found'\n",
    "        print('Phone number not found')\n",
    "    person_info.append(phone)\n",
    "\n",
    "    # find the office and add it to the list\n",
    "    office_div = soup.find('div', {'class': 'office'})\n",
    "    if office_div:\n",
    "        office = office_div.text.strip()\n",
    "    else:\n",
    "        office = 'Not Found'\n",
    "        print('Office not found')\n",
    "    person_info.append(office)\n",
    "\n",
    "    # write the person_info list to a row in the CSV file\n",
    "    with open('people.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(person_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jeff Erickson\\n': 'Vikram Adve\\n', 'Computer Science': 'Computer Science', 'UIUC': 'UIUC', 'Ph.D., Computer Science, University of California, Berkeley, July 1996\\nM.S., Information and Computer Science, University of California, Irvine, June 1992\\nB.A., Computer Science and Mathematical Sciences (double major), Rice University, May 1987': 'Ph.D. in Computer Science from University of Wisconsin-Madison, 1993.', 'Sohaib and Sara Abbasi Professor, University of Illinois at Urbana-Champaign, 2020-present\\nProfessor, University of Illinois at Urbana-Champaign, 2010-present\\nAssociate Professor (tenured), University of Illinois at Urbana-Champaign, 2004-2010\\nAssistant Professor, University of Illinois at Urbana-Champaign, 1998-2004': 'Professor, Coordinated Science Laboratory, August 2011-present, 0%\\nProfessor, Information Trust Institute, August 2011-present, 0%\\nProfessor, Computer Science Department, Univ. of Illinois, August 2011-present, 100%', 'Algorithms, data structures, and lower bounds\\nComputational and discrete geometry and topology': 'Not Found', 'jeffe@illinois.edu': 'vadve@illinois.edu', '(217) 333-6769': '(217) 244-2016', '3237 Siebel Center for Comp Sci': '4235 Siebel Center for Comp Sci'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('people.csv', mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_title(soup):\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        return title_parts[0], title_parts[1], title_parts[2]\n",
    "    else:\n",
    "        return '', '', ''\n",
    "\n",
    "def extract_contact_info(soup):\n",
    "    email_div = soup.find('div', class_='email')\n",
    "    email = email_div.a.text if email_div else ''\n",
    "    phone_div = soup.find('div', class_='phone')\n",
    "    phone = phone_div.text.strip() if phone_div else ''\n",
    "    office_div = soup.find('div', class_='office')\n",
    "    office = office_div.text.strip() if office_div else ''\n",
    "    return email, phone, office\n",
    "\n",
    "def extract_list_info(soup, header_text, label):\n",
    "    h2 = soup.find('h2', text=header_text)\n",
    "    if h2:\n",
    "        ul = h2.find_next('ul')\n",
    "        if ul:\n",
    "            return [f\"{label}: {li.text.strip()}\" for li in ul.find_all('li')]\n",
    "        else:\n",
    "            return [f\"{label}: Not Found\"]\n",
    "    else:\n",
    "        return [f\"{label}: Not Found\"]\n",
    "\n",
    "def extract_person_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name, department, university = extract_title(soup)\n",
    "    email, phone, office = extract_contact_info(soup)\n",
    "\n",
    "    h2_headers = [(\"Education\", \"Education\"), (\"Academic Positions\", \"Academic Positions\"), (\"Research Interests\", \"Research Interests\")]\n",
    "    person_info = [f\"Name: {name}\", f\"Department: {department}\", f\"University: {university}\", f\"Email: {email}\", f\"Phone: {phone}\", f\"Office: {office}\"]\n",
    "\n",
    "    for header_text, label in h2_headers:\n",
    "        person_info += extract_list_info(soup, header_text, label)\n",
    "\n",
    "    return person_info\n",
    "\n",
    "def write_person_info(person_info):\n",
    "    with open('people.txt', 'a') as file:\n",
    "        file.write('\\n'.join(person_info) + \"\\n<----------------------------------------------------------------->\\n\")\n",
    "\n",
    "#profs = ['barford', 'zilles', 'lillis', 'aluru', 'fagen', 'mccoy', 'rsantrac', 'atchley', 'wgropp']\n",
    "\n",
    "for prof in profs:\n",
    "    url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "    person_info = extract_person_info(url)\n",
    "    write_person_info(person_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_title(soup):\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        return title_parts[0], title_parts[1], title_parts[2]\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "def extract_contact_info(soup):\n",
    "    email_div = soup.find('div', class_='email')\n",
    "    email = email_div.a.text if email_div else ''\n",
    "    phone_div = soup.find('div', class_='phone')\n",
    "    phone = phone_div.text.strip() if phone_div else ''\n",
    "    office_div = soup.find('div', class_='office')\n",
    "    office = office_div.text.strip() if office_div else ''\n",
    "    return email, phone, office\n",
    "\n",
    "def extract_list_info(soup, header_text, label):\n",
    "    h2 = soup.find('h2', text=header_text)\n",
    "    if h2:\n",
    "        ul = h2.find_next('ul')\n",
    "        if ul:\n",
    "            return [f\"{label}: {li.text.strip()}\" for li in ul.find_all('li')]\n",
    "        else:\n",
    "            return [f\"{label}: Not Found\"]\n",
    "    else:\n",
    "        return [f\"{label}: Not Found\"]\n",
    "\n",
    "def extract_person_info(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name, department, university = extract_title(soup)\n",
    "    if name is None:\n",
    "        print(f\"Error: Could not extract name from {url}\")\n",
    "        return None\n",
    "\n",
    "    email, phone, office = extract_contact_info(soup)\n",
    "\n",
    "    h2_headers = [(\"Education\", \"Education\"), (\"Academic Positions\", \"Academic Positions\"), (\"Research Interests\", \"Research Interests\")]\n",
    "    person_info = [f\"Name: {name}\", f\"Department: {department}\", f\"University: {university}\", f\"Email: {email}\", f\"Phone: {phone}\", f\"Office: {office}\"]\n",
    "\n",
    "    for header_text, label in h2_headers:\n",
    "        person_info += extract_list_info(soup, header_text, label)\n",
    "\n",
    "    return person_info\n",
    "\n",
    "def write_person_info(person_info):\n",
    "    with open('people.txt', 'a') as file:\n",
    "        file.write('\\n'.join(person_info) + \"\\n<----------------------------------------------------------------->\\n\")\n",
    "\n",
    "\n",
    "for prof in profs:\n",
    "    url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "    person_info = extract_person_info(url)\n",
    "    if person_info is not None:\n",
    "        write_person_info(person_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "tree = etree.fromstring(str(soup))\n",
    "\n",
    "# Convert the tree to a string representation\n",
    "tree_str = etree.tostring(tree, pretty_print=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/qnw8sx3d3rvfffbjccjjr9l00000gn/T/ipykernel_79626/1014550194.py:48: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  linkage_matrix = linkage(dist_matrix, method='ward')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract DOM tree from HTML page\n",
    "def extract_dom_tree(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# Compare DOM trees using Jaccard distance\n",
    "def jaccard_distance(soup1, soup2):\n",
    "    set1 = set(str(soup1).split())\n",
    "    set2 = set(str(soup2).split())\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return 1 - (intersection / union)\n",
    "\n",
    "# Fetch and extract DOM tree for each URL\n",
    "urls = [\n",
    "    \"https://cs.illinois.edu/about/people/all-faculty/jeffe\",\n",
    "    \"https://cs.illinois.edu/about/people/all-faculty/vadve\",\n",
    "]\n",
    "dom_trees = []\n",
    "for url in urls:\n",
    "    dom_trees.append(extract_dom_tree(url))\n",
    "\n",
    "# Compute distance matrix\n",
    "dist_matrix = []\n",
    "for i in range(len(dom_trees)):\n",
    "    row = []\n",
    "    for j in range(len(dom_trees)):\n",
    "        if i == j:\n",
    "            row.append(0)\n",
    "        else:\n",
    "            distance = jaccard_distance(dom_trees[i], dom_trees[j])\n",
    "            row.append(distance)\n",
    "    dist_matrix.append(row)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(dist_matrix, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(linkage_matrix, labels=urls)\n",
    "plt.ylabel('Jaccard distance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (946,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m root \u001b[39m=\u001b[39m cluster_dom_tree(soup)\n",
      "\u001b[1;32m/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb Cell 12\u001b[0m in \u001b[0;36mcluster_dom_tree\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m features \u001b[39m=\u001b[39m [extract_features(node) \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m nodes]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Cluster the features using KMeans\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Assign each node to a cluster based on its features\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m clusters \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1137\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1112\u001b[0m     \u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \n\u001b[1;32m   1114\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1138\u001b[0m         X,\n\u001b[1;32m   1139\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1140\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1141\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1142\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy_x,\n\u001b[1;32m   1143\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[1;32m   1146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(X)\n\u001b[1;32m   1147\u001b[0m     random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 566\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    567\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[1;32m    568\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    745\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    747\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    748\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    749\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    750\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (946,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Function to extract features from a DOM node\n",
    "def extract_features(node):\n",
    "    features = []\n",
    "    features.append(node.name)\n",
    "    features += [f\"{k}={v}\" for k, v in node.attrs.items()]\n",
    "    features.append(node.text.strip())\n",
    "    return features\n",
    "\n",
    "# Function to cluster a DOM tree using KMeans\n",
    "def cluster_dom_tree(root):\n",
    "    # Extract features from all nodes in the DOM tree\n",
    "    nodes = [node for node in root.descendants if node.name is not None]\n",
    "    features = [extract_features(node) for node in nodes]\n",
    "    # Cluster the features using KMeans\n",
    "    kmeans = KMeans(n_clusters=3, random_state=0).fit(features)\n",
    "    # Assign each node to a cluster based on its features\n",
    "    clusters = kmeans.labels_\n",
    "    for i, node in enumerate(nodes):\n",
    "        node['cluster'] = clusters[i]\n",
    "    return root\n",
    "\n",
    "# Example usage\n",
    "url = \"https://cs.illinois.edu/about/people/all-faculty/jeffe\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "root = cluster_dom_tree(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "\n",
    "# specify the URL of the webpage to be scraped\n",
    "for prof in profs:\n",
    "    url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "\n",
    "    # specify the list of h2 headers to search for and the corresponding text labels\n",
    "\n",
    "    # send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content of the webpage using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # extract the text content of the webpage\n",
    "    text = soup.text\n",
    "\n",
    "    # initialize a pipeline to extract named entities using a pre-trained transformer model\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")\n",
    "\n",
    "    # extract named entities from the text\n",
    "    entities = ner_pipeline(text)\n",
    "\n",
    "    # search for the Education entity and extract its value\n",
    "    for entity in entities:\n",
    "        if entity['entity'] == 'EDUCATION':\n",
    "            education = entity['word']\n",
    "            print(f\"Education: {education}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week2_tasks/data_mining_t2.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ner_pipeline \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mner\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdslim/bert-base-NER\u001b[39m\u001b[39m\"\u001b[39m, tokenizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdslim/bert-base-NER\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer#, BertForTokenClassification\n",
    "#from transformers import pipeline\n",
    "# initialize a tokenizer and model to extract named entities using BERT\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "#model = BertForTokenClassification.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# encode the text using the tokenizer\n",
    "#tokens = tokenizer.encode(text, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88b0272d80acab83776fd72d1a9b5c7096c825809c4fd3f61ad7b313653bb5a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
