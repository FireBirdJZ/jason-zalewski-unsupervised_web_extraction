{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input-group', 'roles', 'tile-list', 'title', 'col-12', 'w-100', 'hidden', 'blocki', 'office', 'extProfileAREA', 'site_name', 'flex', 'lower', 'd-none', 'collapse', 'white-box', 'h1', 'site_identification', 'col-lg-7', 'email', 'row', 'input-group-append', 'menucol', 'p-3', 'description', 'directory-profile', 'campus_wordmark', 'recent-post-photo', 'col-lg-5', 'h3', 'container-fluid', 'role', 'parent_name', 'col', 'col-md', 'col-md-auto', 'dropdown-menu', 'contact', 'phone', 'h2'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get('https://cs.illinois.edu/about/people/all-faculty/jeffe')\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Create an empty set to store the headers and class divs\n",
    "dom_set = set()\n",
    "\n",
    "# Find all headers and class divs and add them to the set\n",
    "for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "    dom_set.add(header.name)\n",
    "\n",
    "for div in soup.find_all('div', {'class': True}):\n",
    "    dom_set.add(div['class'][0])\n",
    "\n",
    "# Print the set to check the results\n",
    "print(dom_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get('https://cs.illinois.edu/about/people/all-faculty/jeffe')\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Create an empty dictionary to store the headers and class divs and their descendants\n",
    "dom_dict = {}\n",
    "\n",
    "def get_descendants(elem):\n",
    "    descendants = set()\n",
    "    for child in elem.children:\n",
    "        if isinstance(child, NavigableString):\n",
    "            descendants.add(str(child))\n",
    "        elif child.name and child.name.startswith('h') or child.name == 'div' and 'class' in child.attrs:\n",
    "            break\n",
    "        else:\n",
    "            descendants.add(str(child))\n",
    "            descendants |= get_descendants(child)\n",
    "    return descendants\n",
    "\n",
    "\n",
    "# Find all headers and class divs and their descendants and store them in the dictionary\n",
    "for header in soup.find_all(['h2']):\n",
    "    dom_dict[str(header)] = get_descendants(header)\n",
    "\n",
    "for div in soup.find_all('div', {'class': True}):\n",
    "    dom_dict[div['class'][0]] = get_descendants(div)\n",
    "\n",
    "# Print the dictionary to check the results\n",
    "#print(dom_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research :\n",
      "{'Explore visionary research conducted by world-renowned faculty.'}\n",
      "{'Help ensure that Illinois continues to set a global standard for CS research and education.'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import requests\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get('https://cs.illinois.edu/about/people/all-faculty/jeffe')\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Create a dictionary to store the headers and class divs and their descendants\n",
    "dom_dict = {}\n",
    "\n",
    "def get_descendants(elem):\n",
    "    descendants = set()\n",
    "    for child in elem.children:\n",
    "        if isinstance(child, NavigableString):\n",
    "            descendants.add(str(child))\n",
    "        elif child.name and child.name.startswith('h') or child.name == 'div' and 'class' in child.attrs:\n",
    "            break\n",
    "        else:\n",
    "            descendants.add(str(child))\n",
    "            descendants |= get_descendants(child)\n",
    "    return descendants\n",
    "\n",
    "# Find all headers and class divs containing the word \"research\" and their descendants and group them together\n",
    "for header in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "    header_text = header.get_text().strip()\n",
    "    if 'research' in header_text or ('class' in header.attrs and any('research' in x.lower() for x in header['class'])):\n",
    "        dom_dict.setdefault('Research', []).append(get_descendants(header))\n",
    "\n",
    "for div in soup.find_all('div', {'class': True}):\n",
    "    if 'research' in div['class'][0].lower():\n",
    "        dom_dict.setdefault('Research', []).append(get_descendants(div))\n",
    "\n",
    "\n",
    "# Print the dictionary to check the results\n",
    "for key, value in dom_dict.items():\n",
    "    print(key, ':')\n",
    "    for elem in value:\n",
    "        print(elem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['About', 'Admissions', 'Academics', 'Research', 'News', 'Broadening Participation', 'Help ensure that Illinois continues to set a global standard for CS research and education.', 'Give', 'Jeff Erickson', 'For More Information', 'Education', 'Biography', 'Teaching Statement', 'Research Statement', 'Research Interests', 'Research Areas', 'Selected Articles in Journals', 'Conferences Organized or Chaired', 'Teaching Honors', 'Research Honors', 'Recent Courses Taught', 'Related News', 'Software: It Is All In The Details']\n",
      "Cluster 1: ['Grainger Engineering Investitures Honor Seven Distinguished CS Faculty']\n",
      "Cluster 2: ['Explore visionary research conducted by world-renowned faculty.']\n",
      "Cluster 3: ['Ready to apply? Your path to CS at Illinois begins here.', 'Ready to apply? Your path to CS at Illinois begins here.']\n",
      "Cluster 4: ['Surprise Computer Science Proof Stuns Mathematicians']\n",
      "Cluster 5: ['Illinois CS Hosts First-Of-Its-Kind NSF Workshop on Departmental Plans for Broadening Participation in Computing']\n",
      "Cluster 6: ['Student Life']\n",
      "Cluster 7: ['Books Authored or Co-Authored (Original Editions)']\n",
      "Cluster 8: ['Academic Positions']\n",
      "Cluster 9: ['Articles in Conference Proceedings']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send GET request to the webpage\n",
    "url = 'https://cs.illinois.edu/about/people/all-faculty/jeffe'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse HTML content using BeautifulSoup\n",
    "html_content = response.content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the header text using regular expressions\n",
    "header_text = []\n",
    "for header in soup.find_all(re.compile('^h[1-6]$')):\n",
    "    header_text.append(header.text.strip())\n",
    "\n",
    "# Convert the header text to a document-term matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(header_text)\n",
    "\n",
    "# Apply K-means clustering\n",
    "k = 10\n",
    "km = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "km.fit(X)\n",
    "\n",
    "# Print the cluster labels\n",
    "labels = km.labels_\n",
    "for i in range(k):\n",
    "    print(\"Cluster {}: {}\".format(i, [header_text[j] for j in np.where(labels == i)[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education:\n",
      "- Ph.D., Computer Science, University of California, Berkeley, July 1996\n",
      "- M.S., Information and Computer Science, University of California, Irvine, June 1992\n",
      "- B.A., Computer Science and Mathematical Sciences (double major), Rice University, May 1987\n",
      "\n",
      "Academic Positions:\n",
      "- Sohaib and Sara Abbasi Professor, University of Illinois at Urbana-Champaign, 2020-present\n",
      "- Professor, University of Illinois at Urbana-Champaign, 2010-present\n",
      "- Associate Professor (tenured), University of Illinois at Urbana-Champaign, 2004-2010\n",
      "- Assistant Professor, University of Illinois at Urbana-Champaign, 1998-2004\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# assume that the html string is stored in a variable called 'html'\n",
    "url = 'https://cs.illinois.edu/about/people/all-faculty/jeffe'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find the education section\n",
    "education_section = soup.find('h2', text='Education')\n",
    "if education_section:\n",
    "    # get the list of education items\n",
    "    education_list = education_section.find_next('ul')\n",
    "    if education_list:\n",
    "        # extract each education item as a string\n",
    "        education_items = [item.text for item in education_list.find_all('li')]\n",
    "        print('Education:')\n",
    "        for item in education_items:\n",
    "            print('- ' + item)\n",
    "\n",
    "# find the academic positions section\n",
    "positions_section = soup.find('h2', text='Academic Positions')\n",
    "if positions_section:\n",
    "    # get the list of academic positions\n",
    "    positions_list = positions_section.find_next('ul')\n",
    "    if positions_list:\n",
    "        # extract each academic position as a string\n",
    "        positions_items = [item.text for item in positions_list.find_all('li')]\n",
    "        print('\\nAcademic Positions:')\n",
    "        for item in positions_items:\n",
    "            print('- ' + item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_section(html, section_title):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    section = soup.find('h2', text=section_title)\n",
    "    if section is None:\n",
    "        return None\n",
    "    data = section.find_next_sibling()\n",
    "    if data is None:\n",
    "        return None\n",
    "    if data.name == 'ul':\n",
    "        results = []\n",
    "        for li in data.find_all('li'):\n",
    "            results.append(li.get_text())\n",
    "        return results\n",
    "    else:\n",
    "        return data.get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ph.D., Computer Science, University of California, Berkeley, July 1996', 'M.S., Information and Computer Science, University of California, Irvine, June 1992', 'B.A., Computer Science and Mathematical Sciences (double major), Rice University, May 1987']\n",
      "['Algorithms, data structures, and lower bounds', 'Computational and discrete geometry and topology']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# for the 'Education' section\n",
    "education_section = extract_section(html, 'Education')\n",
    "\n",
    "# for the 'Research Interests' section\n",
    "research_interests_section = extract_section(html, 'Research Interests')\n",
    "\n",
    "# for the 'Contact Information' section\n",
    "contact_information_section = extract_section(html, 'Contact Information')\n",
    "\n",
    "print(education_section)\n",
    "print(research_interests_section)\n",
    "print(contact_information_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Education: <ul>\n",
      "<li>Ph.D., Computer Science, University of California, Berkeley, July 1996</li>\n",
      "<li>M.S., Information and Computer Science, University of California, Irvine, June 1992</li>\n",
      "<li>B.A., Computer Science and Mathematical Sciences (double major), Rice University, May 1987</li>\n",
      "</ul>\n",
      "Found Research Interests: <ul>\n",
      "<li>Algorithms, data structures, and lower bounds</li>\n",
      "<li>Computational and discrete geometry and topology</li>\n",
      "</ul>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#url = 'https://cs.illinois.edu/about/people/all-faculty/jeffe'\n",
    "#url = 'https://chemistry.illinois.edu/mikaelb'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "# assuming you have the html stored in the variable 'html'\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# define a list of regex patterns to match the headers you're interested in\n",
    "patterns = ['Education', 'Research Interests', 'Contact Information']\n",
    "\n",
    "# loop through all h2 tags and check if they match any of the patterns\n",
    "for h2 in soup.find_all('h2'):\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, h2.text):\n",
    "            print(f\"Found {pattern}: {h2.next_sibling}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No contact info found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# specify the URL of the webpage to be scraped\n",
    "for prof in profs:\n",
    "    url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "\n",
    "    try:\n",
    "        # send a GET request to the webpage\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        print('Did not find:', url)\n",
    "        continue\n",
    "\n",
    "    # parse the HTML content of the webpage using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # extract the name, department, and university from the webpage title\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        name = title_parts[0]\n",
    "        department = title_parts[1]\n",
    "        university = title_parts[2]\n",
    "    else:\n",
    "        print('No title tag found')\n",
    "        name = ''\n",
    "        department = ''\n",
    "        university = ''\n",
    "\n",
    "    # extract the email, phone number, and office location from the webpage\n",
    "    contact_info = soup.find('div', class_='contact-info')\n",
    "    if contact_info:\n",
    "        email = contact_info.find('a', href='mailto:').text if contact_info.find('a', href='mailto:') else ''\n",
    "        phone = contact_info.find('a', href=lambda href: href and href.startswith('tel:')).text.strip() if contact_info.find('a', href=lambda href: href and href.startswith('tel:')) else ''\n",
    "        office = contact_info.find('a', href=lambda href: href and 'maps.google.com' in href).text.strip() if contact_info.find('a', href=lambda href: href and 'maps.google.com' in href) else ''\n",
    "    else:\n",
    "        print('No contact info found')\n",
    "        email = ''\n",
    "        phone = ''\n",
    "        office = ''\n",
    "\n",
    "    # create a string to hold the information of the new person\n",
    "    person_info = f\"Name: {name}\\nDepartment: {department}\\nUniversity: {university}\\nEmail: {email}\\nPhone: {phone}\\nOffice: {office}\\n\\n\"\n",
    "\n",
    "    # find all the headers (h2 tags) on the page\n",
    "    headers = soup.find_all('h2')\n",
    "\n",
    "    # loop through each header and look for the corresponding information\n",
    "    for header in headers:\n",
    "        header_text = header.text.strip()\n",
    "        if header_text == 'Education':\n",
    "            # find the next sibling element and add it to the person_info string\n",
    "            education_info = header.find_next_sibling()\n",
    "            if education_info:\n",
    "                person_info += f\"Education: {education_info.text.strip()}\\n\"\n",
    "            else:\n",
    "                print('No education info found')\n",
    "                person_info += f\"Education: Not Found\\n\"\n",
    "        elif header_text == 'Academic Positions':\n",
    "            # find the next sibling element and add it to the person_info string\n",
    "            position_info = header.find_next_sibling()\n",
    "            if position_info:\n",
    "                person_info += f\"Academic Positions: {position_info.text.strip()}\\n\"\n",
    "            else:\n",
    "                print('No academic positions found')\n",
    "                person_info += f\"Academic Positions: Not Found\\n\"\n",
    "        elif header_text == 'Research Interests':\n",
    "            # find the next sibling element and add it to the person_info string\n",
    "            interest_info = header.find_next_sibling()\n",
    "            if interest_info:\n",
    "                person_info += f\"Research Interests: {interest_info.text.strip()}\\n\"\n",
    "            else:\n",
    "                print('No research interests found')\n",
    "                person_info += f\"Research Interests: Not Found\\n\"\n",
    "        else:\n",
    "            # ignore all other headers\n",
    "            pass\n",
    "\n",
    "    # write the person_info string\n",
    "    with open('peopletest.txt', 'a') as file:\n",
    "        file.write(person_info + \"\\n<----------------------------------------------------------------->\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m url \u001b[39m=\u001b[39m  \u001b[39m'\u001b[39m\u001b[39mhttps://chemistry.illinois.edu/mikaelb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m#url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m person_info \u001b[39m=\u001b[39m extract_person_info(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mif\u001b[39;00m person_info \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m      write_person_info(person_info)\n",
      "\u001b[1;32m/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb Cell 11\u001b[0m in \u001b[0;36mextract_person_info\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m name, department, university \u001b[39m=\u001b[39m extract_title(soup)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: Could not extract name from \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb Cell 11\u001b[0m in \u001b[0;36mextract_title\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mif\u001b[39;00m title:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     title_parts \u001b[39m=\u001b[39m title\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m | \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m title_parts[\u001b[39m0\u001b[39m], title_parts[\u001b[39m1\u001b[39m], title_parts[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jasonz/Desktop/ForwardDataLabResearch/task1_datamine/Week3_tasks/data_mining_t3.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "profs = ['jeffe']\n",
    "\n",
    "\n",
    "def extract_title(soup):\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        return title_parts[0], title_parts[1], title_parts[2]\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "def extract_contact_info(soup):\n",
    "    email_div = soup.find('div', class_='email')\n",
    "    email = email_div.a.text if email_div else ''\n",
    "    phone_div = soup.find('div', class_='phone')\n",
    "    phone = phone_div.text.strip() if phone_div else ''\n",
    "    office_div = soup.find('div', class_='office')\n",
    "    office = office_div.text.strip() if office_div else ''\n",
    "    return email, phone, office\n",
    "\n",
    "def extract_list_info(soup, header_text, label):\n",
    "    h2 = soup.find('h2', text=header_text)\n",
    "    if h2:\n",
    "        ul = h2.find_next('ul')\n",
    "        if ul:\n",
    "            return [f\"{label}: {li.text.strip()}\" for li in ul.find_all('li')]\n",
    "        else:\n",
    "            return [f\"{label}: Not Found\"]\n",
    "    else:\n",
    "        return [f\"{label}: Not Found\"]\n",
    "\n",
    "def extract_person_info(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name, department, university = extract_title(soup)\n",
    "    if name is None:\n",
    "        print(f\"Error: Could not extract name from {url}\")\n",
    "        return None\n",
    "\n",
    "    email, phone, office = extract_contact_info(soup)\n",
    "\n",
    "    headers = soup.find_all('h2')\n",
    "    person_info = [f\"Name: {name}\", f\"Department: {department}\", f\"University: {university}\", f\"Email: {email}\", f\"Phone: {phone}\", f\"Office: {office}\"]\n",
    "    current_header = None\n",
    "\n",
    "    for header in headers:\n",
    "        header_text = header.text.strip()\n",
    "        ul = header.find_next('ul')\n",
    "        if ul:\n",
    "            items = [li.text.strip() for li in ul.find_all('li')]\n",
    "            if header_text != current_header:\n",
    "                person_info.append(header_text)\n",
    "                current_header = header_text\n",
    "            for item in items:\n",
    "                person_info.append(f\"{header_text}: {item}\")\n",
    "\n",
    "    return person_info\n",
    "\n",
    "\n",
    "def write_person_info(person_info):\n",
    "    with open('people.txt', 'a') as file:\n",
    "        file.write('\\n'.join(person_info) + \"\\n<----------------------------------------------------------------->\\n\")\n",
    "\n",
    "\n",
    "for prof in profs:\n",
    "    url =  'https://chemistry.illinois.edu/mikaelb'\n",
    "    #url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "    person_info = extract_person_info(url)\n",
    "    if person_info is not None:\n",
    "         write_person_info(person_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not extract name from https://chemistry.illinois.edu/mikaelb\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "profs = ['jeffe']\n",
    "\n",
    "def extract_title(soup):\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        if len(title_parts) >= 3:\n",
    "            return title_parts[0], title_parts[1], title_parts[2]\n",
    "    return None, None, None\n",
    "\n",
    "def extract_contact_info(soup):\n",
    "    email_div = soup.find('div', class_='email')\n",
    "    email = email_div.a.text if email_div else ''\n",
    "    phone_div = soup.find('div', class_='phone')\n",
    "    phone = phone_div.text.strip() if phone_div else ''\n",
    "    office_div = soup.find('div', class_='office')\n",
    "    office = office_div.text.strip() if office_div else ''\n",
    "    return email, phone, office\n",
    "\n",
    "def extract_list_info(soup, headers):\n",
    "    info = {}\n",
    "    for header_text, label in headers:\n",
    "        h2 = soup.find('h2', text=header_text)\n",
    "        if h2:\n",
    "            ul = h2.find_next('ul')\n",
    "            if ul:\n",
    "                info[label] = [li.text.strip() for li in ul.find_all('li')]\n",
    "            else:\n",
    "                info[label] = None\n",
    "        else:\n",
    "            info[label] = None\n",
    "    return info\n",
    "\n",
    "def extract_person_info(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name, department, university = extract_title(soup)\n",
    "    if name is None:\n",
    "        print(f\"Error: Could not extract name from {url}\")\n",
    "        return None\n",
    "\n",
    "    email, phone, office = extract_contact_info(soup)\n",
    "\n",
    "    info = extract_list_info(soup, headers)\n",
    "\n",
    "    person_info = [f\"Name: {name}\", f\"Department: {department}\", f\"University: {university}\", f\"Email: {email}\", f\"Phone: {phone}\", f\"Office: {office}\"]\n",
    "\n",
    "    for label, value in info.items():\n",
    "        if value is not None:\n",
    "            person_info.append(f\"{label}:\")\n",
    "            person_info.extend([f\"\\t- {v}\" for v in value])\n",
    "\n",
    "    return person_info\n",
    "\n",
    "def write_person_info(person_info):\n",
    "    with open('peopletest.txt', 'a') as file:\n",
    "        file.write('\\n'.join(person_info) + \"\\n<----------------------------------------------------------------->\\n\")\n",
    "\n",
    "\n",
    "headers = [\n",
    "    (\"Education\", \"Education\"),\n",
    "    (\"Academic Positions\", \"Academic Positions\"),\n",
    "    (\"Research Interests\", \"Research Interests\"),\n",
    "    (\"Selected Publications\", \"Selected Publications\"),\n",
    "    (\"Honors\", \"Honors and Awards\"),\n",
    "    (\"Service\", \"Professional Service\"),\n",
    "]\n",
    "\n",
    "\n",
    "for prof in profs:\n",
    "    url =  'https://chemistry.illinois.edu/mikaelb'\n",
    "    #url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "    person_info = extract_person_info(url, headers)\n",
    "    if person_info is not None:\n",
    "        write_person_info(person_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# cs_prof\n",
    "#profs = ['jeffe', 'hanj', 'zaher']\n",
    "\n",
    "# chem_prof\n",
    "profs = ['mikaelb', 'mdburke', 'agewirth']\n",
    "def extract_title(soup):\n",
    "    title = soup.head.find('title')\n",
    "    if title:\n",
    "        title_parts = title.text.strip().split(' | ')\n",
    "        if len(title_parts) >= 2:\n",
    "            name = title_parts[0]\n",
    "            department = title_parts[1].split(' - ')[0]\n",
    "            university = 'University of Illinois at Urbana-Champaign'\n",
    "            return name, department, university\n",
    "    h1 = soup.find('h1', class_='page-title')\n",
    "    if h1:\n",
    "        name = h1.text.strip()\n",
    "        department_div = soup.find('div', class_='field--name-field-primary-department')\n",
    "        department = department_div.text.strip() if department_div else None\n",
    "        university = 'University of Illinois at Urbana-Champaign'\n",
    "        return name, department, university\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def extract_contact_info(soup):\n",
    "    email = ''\n",
    "    email_div = soup.find('div', class_='email')\n",
    "    if email_div:\n",
    "        email = email_div.a.text.strip()\n",
    "    else:\n",
    "        email_match = re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', str(soup))\n",
    "        if email_match:\n",
    "            email = email_match.group(0)\n",
    "    \n",
    "    phone_div = soup.find('div', class_='phone')\n",
    "    phone = phone_div.text.strip() if phone_div else ''\n",
    "    if not phone:\n",
    "        phone_tags = soup.find_all('a', href=re.compile(r'^tel:'))\n",
    "        if phone_tags:\n",
    "            phone = phone_tags[0].text.strip()\n",
    "    \n",
    "    office_div = soup.find('div', class_='office')\n",
    "    office = office_div.text.strip() if office_div else ''\n",
    "\n",
    "    # If office is not found, get all <br> tags below 'Contact Information'\n",
    "    if not office:\n",
    "        contact_info = soup.find('h2', text='Contact Information').find_next('div')\n",
    "        office_tags = contact_info.find_all('br')\n",
    "        office = ' '.join([tag.next_sibling.strip() for tag in office_tags if tag.next_sibling])\n",
    "    \n",
    "    return email, phone, office\n",
    "\n",
    "def extract_list_info(soup, headers):\n",
    "    info = {}\n",
    "    for header_text, label in headers:\n",
    "        h2 = soup.find('h2', text=header_text)\n",
    "        if h2:\n",
    "            ul = h2.find_next('ul')\n",
    "            if ul:\n",
    "                items = [li.text.strip() for li in ul.find_all('li') if 'Additional resources' not in li.text]\n",
    "                info[label] = items if items else None\n",
    "            else:\n",
    "                info[label] = None\n",
    "        else:\n",
    "            h2 = soup.find('h2', class_='profile-label', text='Research Interests')\n",
    "            if h2:\n",
    "                p = h2.find_next_sibling('div').find('p')\n",
    "                if p:\n",
    "                    info['Research Areas'] = [p.text.strip()]\n",
    "                else:\n",
    "                    p = h2.find_next_sibling('p')\n",
    "                    if p:\n",
    "                        info['Research Areas'] = [p.text.strip()]\n",
    "                    else:\n",
    "                        info['Research Areas'] = None\n",
    "            else:\n",
    "                info[label] = None\n",
    "    return info\n",
    "\n",
    "def extract_person_info(url, headers):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    name, department, university = extract_title(soup)\n",
    "    if name is None:\n",
    "        print(f\"Error: Could not extract name from {url}\")\n",
    "        return []\n",
    "\n",
    "    email, phone, office = extract_contact_info(soup)\n",
    "\n",
    "    info = extract_list_info(soup, headers)\n",
    "\n",
    "    person_info = [f\"Name: {name}\", f\"Department: {department}\", f\"University: {university}\", f\"Email: {email}\", f\"Phone: {phone}\", f\"Office: {office}\"]\n",
    "\n",
    "    for label, value in info.items():\n",
    "        if value is not None:\n",
    "            person_info.append(f\"{label}:\")\n",
    "            person_info.extend([f\"\\t- {v}\" for v in value])\n",
    "\n",
    "    return person_info\n",
    "\n",
    "def write_person_info(person_info):\n",
    "    if person_info == \"invalid_url\":\n",
    "        with open('people.txt', 'a') as file:\n",
    "            file.write(\"invalid_url\\n\")\n",
    "    else:\n",
    "        with open('people.txt', 'a') as file:\n",
    "            file.write('\\n'.join(person_info) + \"\\n<----------------------------------------------------------------->\\n\")\n",
    "\n",
    "\n",
    "headers = [\n",
    "(\"Education\", \"Education\"),\n",
    "(\"Research Areas\", \"Research Areas\"),\n",
    "(\"Selected Publications\", \"Selected Publications\"),\n",
    "(\"Selected Talks\", \"Selected Talks\")\n",
    "]\n",
    "\n",
    "for prof in profs:\n",
    "    url = f\"https://chemistry.illinois.edu/{prof}\"\n",
    "    #url = f\"https://cs.illinois.edu/about/people/all-faculty/{prof}\"\n",
    "    person_info = extract_person_info(url, headers)\n",
    "    if person_info:\n",
    "        write_person_info(person_info)\n",
    "    else:\n",
    "        print(f\"Error: Could not extract person info from {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88b0272d80acab83776fd72d1a9b5c7096c825809c4fd3f61ad7b313653bb5a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
